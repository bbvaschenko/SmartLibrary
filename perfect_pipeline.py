
"""
–ò–î–ï–ê–õ–¨–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ê–ù–ê–õ–ò–ó–ê –ö–ù–ò–ì
–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö 6 –º–µ—Ç—Ä–∏–∫ —Å –±–µ—Å—à–æ–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É
"""

import json
import logging
import os
import time
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections import defaultdict, Counter
import random
import hashlib
from dataclasses import dataclass, asdict, field
from enum import Enum

# –ò–º–ø–æ—Ä—Ç –≤–∞—à–µ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º—ã
from agents.gigachat_client import GigaChatClient
from agents.coordinator_agent import CoordinatorAgent
from agents.search_agent import SearchAgent
from agents.analysis_agent import AnalysisAgent
from agents.critic_agent import CriticAgent
from agents.recommendation_agent import RecommendationAgent
from agents.summary_agent import SummaryAgent
from utils.pdf_processor import PDFProcessor
from utils.data_manager import DataManager
from utils.book_tagger import AdvancedBookTagger
from utils.search_tags import TagSearch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ==================== –¢–ò–ü–´ –î–ê–ù–ù–´–• –ò –ú–ï–¢–†–ò–ö–ò ====================

class AgentDecision(Enum):
    """–†–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∞–≥–µ–Ω—Ç—ã"""
    ACCEPT = "accept"
    REJECT = "reject"
    REQUEST_REANALYSIS = "request_reanalysis"
    CONTINUE = "continue"
    COMPLETE = "complete"


@dataclass
class CriticMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ CriticAgent"""
    total_calls: int = 0
    effective_changes: int = 0  # REJECT –∏–ª–∏ REQUEST_REANALYSIS
    acceptance_count: int = 0  # ACCEPT
    explanation_scores: List[float] = field(default_factory=list)  # ECS scores 1-5

    @property
    def cer(self) -> float:
        """Critic Effectiveness Rate"""
        return self.effective_changes / self.total_calls if self.total_calls > 0 else 0.0

    @property
    def average_ecs(self) -> float:
        """Average Explanation Completeness Score"""
        return sum(self.explanation_scores) / len(self.explanation_scores) if self.explanation_scores else 0.0


@dataclass
class PipelineMetrics:
    """–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    # A. –ê–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å
    critic_metrics: CriticMetrics = field(default_factory=CriticMetrics)
    tool_invocations: Dict[str, int] = field(default_factory=dict)

    # B. –ö–∞—á–µ—Å—Ç–≤–æ
    total_queries: int = 0
    accepted_answers: int = 0
    consistency_tests: List[Dict] = field(default_factory=list)

    # C. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å
    errors_detected: int = 0
    errors_recovered: int = 0

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ
    execution_times: List[float] = field(default_factory=list)
    query_types: Counter = field(default_factory=Counter)

    @property
    def aar(self) -> float:
        """Answer Acceptance Rate"""
        return self.accepted_answers / self.total_queries if self.total_queries > 0 else 0.0

    @property
    def frr(self) -> float:
        """Failure Recovery Rate"""
        return self.errors_recovered / self.errors_detected if self.errors_detected > 0 else 1.0

    @property
    def tid(self) -> int:
        """Tool Invocation Diversity"""
        return len([t for t, count in self.tool_invocations.items() if count > 0])

    @property
    def scs(self) -> float:
        """Self Consistency Score (–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–µ—Å—Ç–∞)"""
        if not self.consistency_tests:
            return 0.0
        return self.consistency_tests[-1].get('consistency_score', 0.0)


@dataclass
class ExecutionContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    execution_id: str
    query: str
    start_time: datetime
    iteration: int = 0
    context: Dict[str, Any] = field(default_factory=dict)
    search_results: Optional[List[Dict]] = None
    analysis_result: Optional[Dict] = None
    critique_result: Optional[Dict] = None
    final_decision: Optional[AgentDecision] = None
    processing_time: float = 0.0

    def to_dict(self) -> Dict:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        data = asdict(self)
        data['start_time'] = self.start_time.isoformat()
        data['final_decision'] = self.final_decision.value if self.final_decision else None
        return data


# ==================== –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ü–ê–ô–ü–õ–ê–ô–ù–ê ====================

class PerfectBookAnalysisPipeline:
    """
    –ò–¥–µ–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞–Ω–∞–ª–∏–∑–∞ –∫–Ω–∏–≥ —Å –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
    –∏ –±–µ—Å—à–æ–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É.
    """

    def __init__(
            self,
            base_data_dir: str = "uploads",
            output_dir: str = "pipeline_output",
            verify_ssl: bool = False
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É"""

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.base_data_dir = Path(base_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã...")

        # –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π)
        self.data_manager = DataManager(str(self.base_data_dir))

        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π)
        tags_file = self.base_data_dir / "book_tags.xlsx"
        self.tag_search = TagSearch(str(tags_file))

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ GigaChat
        self.gigachat = GigaChatClient(verify_ssl=verify_ssl)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        self._init_agents()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self._init_tools()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        self.metrics = PipelineMetrics()
        self.metrics_file = self.output_dir / "pipeline_metrics.json"
        self.execution_logs_dir = self.output_dir / "execution_logs"
        self.execution_logs_dir.mkdir(exist_ok=True)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        self._load_metrics()

        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
        self._book_cache = {}
        self._summary_cache = {}

        logger.info(f"‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {len(self.data_manager.get_all_books())} –∫–Ω–∏–≥")

    def _init_agents(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""

        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π)
        self.coordinator = CoordinatorAgent(self.gigachat)

        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–ª–∏–µ–Ω—Ç–∞ GigaChat
        self.search_agent = SearchAgent(self.gigachat)
        self.analysis_agent = AnalysisAgent(verify_ssl=False)
        self.critic_agent = CriticAgent(self.gigachat)
        self.recommendation_agent = RecommendationAgent(self.gigachat)
        self.summary_agent = SummaryAgent(self.gigachat)

        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ
        self.coordinator.register_agent("SearchAgent", self.search_agent)
        self.coordinator.register_agent("AnalysisAgent", self.analysis_agent)
        self.coordinator.register_agent("CriticAgent", self.critic_agent)
        self.coordinator.register_agent("RecommendationAgent", self.recommendation_agent)

        logger.info("‚úÖ –í—Å–µ –∞–≥–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã")

    def _init_tools(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        self.pdf_processor = PDFProcessor()
        self.book_tagger = AdvancedBookTagger()
        logger.info("‚úÖ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    # ==================== –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ ====================

    def process_user_query(
            self,
            query: str,
            context: Optional[Dict] = None,
            max_iterations: int = 3,
            enable_critique: bool = True
    ) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–ª–Ω—ã–º —Ü–∏–∫–ª–æ–º –º–µ—Ç—Ä–∏–∫.

        Args:
            query: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            max_iterations: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            enable_critique: –í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É CriticAgent

        Returns:
            –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        logger.info(f"üîç –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        exec_id = f"exec_{int(time.time())}_{random.randint(1000, 9999)}"
        start_time = datetime.now()

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.metrics.total_queries += 1

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        exec_context = ExecutionContext(
            execution_id=exec_id,
            query=query,
            start_time=start_time
        )

        if context:
            exec_context.context.update(context)

        exec_context.context.update({
            "execution_id": exec_id,
            "original_query": query,
            "enable_critique": enable_critique
        })

        iteration_logs = []
        final_decision = None

        # –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        for iteration in range(max_iterations):
            logger.info(f"üîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{max_iterations}")
            iteration_start = time.time()

            try:
                # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –∫–Ω–∏–≥
                search_results = self._search_books_with_metrics(query, exec_context.context)
                exec_context.search_results = search_results.get("results", [])

                if not exec_context.search_results:
                    logger.warning("üì≠ –ö–Ω–∏–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    break

                # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                book_content = self._get_content_for_analysis(exec_context.search_results[0])
                if not book_content:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                    break

                # –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                analysis_result = self._analyze_content_with_metrics(
                    content=book_content,
                    query=query,
                    context=exec_context.context
                )
                exec_context.analysis_result = analysis_result

                # –®–∞–≥ 4: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
                if enable_critique:
                    critique_result = self._critique_with_metrics(
                        analysis_result=analysis_result,
                        query=query,
                        context=exec_context.context
                    )
                    exec_context.critique_result = critique_result

                    # –û—Ü–µ–Ω–∫–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è (ECS)
                    ecs_score = self._calculate_explanation_score(critique_result)
                    self.metrics.critic_metrics.explanation_scores.append(ecs_score)

                    # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏–∫–∏
                    decision = critique_result.get("decision", {}).get("decision", "ACCEPT")

                    if decision == "ACCEPT":
                        final_decision = AgentDecision.ACCEPT
                        self.metrics.accepted_answers += 1
                        self.metrics.critic_metrics.acceptance_count += 1
                        logger.info("‚úÖ Critic: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–Ω—è—Ç")
                        break

                    elif decision in ["REJECT", "REQUEST_REANALYSIS"]:
                        self.metrics.critic_metrics.effective_changes += 1

                        if decision == "REJECT":
                            final_decision = AgentDecision.REJECT
                            logger.warning("‚ùå Critic: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω")
                            break
                        else:
                            # REQUEST_REANALYSIS - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é
                            logger.info("üîÑ Critic: –∑–∞–ø—Ä–æ—à–µ–Ω –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                            feedback = critique_result.get("critique_results", {}).get("specific_feedback", "")
                            exec_context.context["correction_feedback"] = feedback
                            continue
                else:
                    # –ë–µ–∑ –∫—Ä–∏—Ç–∏–∫–∏ —Å—Ä–∞–∑—É –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    final_decision = AgentDecision.ACCEPT
                    self.metrics.accepted_answers += 1
                    break

            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {e}")
                self.metrics.errors_detected += 1

                # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                if self._recover_from_error(e, exec_context):
                    self.metrics.errors_recovered += 1
                    continue
                else:
                    logger.error(f"‚ùå –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")
                    final_decision = AgentDecision.REJECT
                    break

            iteration_time = time.time() - iteration_start
            logger.info(f"‚è±Ô∏è  –ò—Ç–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {iteration_time:.2f} —Å–µ–∫—É–Ω–¥")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏
            iteration_logs.append({
                "iteration": iteration,
                "duration": iteration_time,
                "has_search_results": bool(exec_context.search_results),
                "has_analysis": bool(exec_context.analysis_result),
                "has_critique": bool(exec_context.critique_result) if enable_critique else False
            })

        # –ï—Å–ª–∏ –Ω–µ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∑–∞ –≤—Å–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏
        if final_decision is None:
            final_decision = AgentDecision.CONTINUE

        # –®–∞–≥ 5: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–Ω—è—Ç)
        final_recommendations = None
        if final_decision == AgentDecision.ACCEPT:
            final_recommendations = self._generate_recommendations_with_metrics(
                analysis_result=exec_context.analysis_result,
                query=query,
                context=exec_context.context
            )

        # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        total_time = time.time() - start_time.timestamp()
        exec_context.processing_time = total_time
        exec_context.final_decision = final_decision

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        self.metrics.execution_times.append(total_time)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = self._create_final_result(
            execution_context=exec_context,
            final_recommendations=final_recommendations,
            iteration_logs=iteration_logs
        )

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        self._save_execution_log(exec_context, iteration_logs, result)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self._update_metrics()
        self._save_metrics()

        logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {total_time:.2f} —Å–µ–∫. –†–µ—à–µ–Ω–∏–µ: {final_decision.value}")

        return result

    def _search_books_with_metrics(self, query: str, context: Dict) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
        logger.info(f"üîé –ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self._record_tool_invocation("SearchAgent")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º SearchAgent –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞
            search_analysis = self.search_agent.process(query, context)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞
            if len(query.split()) <= 3:
                search_method = "tags"
            else:
                search_method = "semantic"

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            if search_method == "tags":
                # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º —á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É
                tags_to_search = self._extract_search_tags(query)
                results = self.tag_search.search_by_tags(tags_to_search, operator="OR")
            else:
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ä–µ–∑—é–º–µ
                results = self._semantic_search(query)

            # –û–±–æ–≥–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            enriched_results = []
            for book in results[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                enriched = self._enrich_book_info(book)
                enriched_results.append(enriched)

            return {
                "query": query,
                "search_method": search_method,
                "results_count": len(results),
                "results": enriched_results,
                "search_analysis": search_analysis,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return {
                "query": query,
                "error": str(e),
                "results_count": 0,
                "results": []
            }

    def _extract_search_tags(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        stop_words = {"–∏—â—É", "–Ω–∞–π—Ç–∏", "–ø–æ–∏—Å–∫", "–∫–Ω–∏–≥—É", "–∫–Ω–∏–≥–∏", "–ø—Ä–æ", "–¥–ª—è", "—Å", "–ø–æ"}
        words = [word.lower() for word in query.split() if word.lower() not in stop_words]
        return words[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤

    def _semantic_search(self, query: str) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ä–µ–∑—é–º–µ –∫–Ω–∏–≥"""
        results = []
        all_books = self.data_manager.get_all_books()

        for book in all_books[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            book_id = book.get('book_id', '')
            summary = self._get_cached_summary(book_id)

            if summary:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                query_terms = set(query.lower().split())
                summary_terms = set(summary.lower().split())
                common_terms = query_terms.intersection(summary_terms)

                if len(common_terms) >= 2:
                    # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    relevance_score = len(common_terms) / len(query_terms) if query_terms else 0
                    book['relevance_score'] = min(relevance_score * 2, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                    results.append(book)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return results

    def _get_content_for_analysis(self, book_info: Dict) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–Ω–∏–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        book_id = book_info.get('book_id', '')

        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        full_text = self._get_cached_book_text(book_id)
        if full_text:
            return full_text[:15000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤

        # –ï—Å–ª–∏ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—é–º–µ
        summary = self._get_cached_summary(book_id)
        if summary:
            return summary

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        return json.dumps({
            "title": book_info.get('title', ''),
            "tags": book_info.get('all_categories', {}),
            "metadata": {k: v for k, v in book_info.items() if k not in ['book_id', 'title']}
        }, ensure_ascii=False)

    def _analyze_content_with_metrics(self, content: str, query: str, context: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
        logger.info("üìä –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self._record_tool_invocation("AnalysisAgent")

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π AnalysisAgent
            result = self.analysis_agent.process(
                content=content,
                topic=query,
                target_audience=context.get('target_audience', 'general'),
                context=context
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞
            if isinstance(result, dict):
                result.update({
                    "analysis_timestamp": datetime.now().isoformat(),
                    "content_length": len(content),
                    "analysis_method": "deep_analysis"
                })

            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {
                "error": str(e),
                "analysis_timestamp": datetime.now().isoformat(),
                "fallback_analysis": True
            }

    def _critique_with_metrics(self, analysis_result: Dict, query: str, context: Dict) -> Dict[str, Any]:
        """–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
        logger.info("‚öñÔ∏è  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞...")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–æ–≤
        self._record_tool_invocation("CriticAgent")
        self.metrics.critic_metrics.total_calls += 1

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π CriticAgent —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            result = self.critic_agent.process(
                query=query,
                analysis_results=analysis_result,
                original_query=query,
                context=context
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if isinstance(result, dict):
                result.update({
                    "critique_timestamp": datetime.now().isoformat(),
                    "critic_version": "1.0"
                })

            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return {
                "error": str(e),
                "decision": {"decision": "ACCEPT"},  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                "critique_timestamp": datetime.now().isoformat()
            }

    def _calculate_explanation_score(self, critique_result: Dict) -> float:
        """–†–∞—Å—á–µ—Ç Explanation Completeness Score (1-5)"""

        score = 3.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞

        try:
            # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: –ù–∞–ª–∏—á–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è
            decision = critique_result.get("decision", {})
            if decision.get("reasoning"):
                score += 0.5

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            critique_results = critique_result.get("critique_results", {})
            if critique_results.get("errors_found") or critique_results.get("missing_aspects"):
                score += 0.5

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: –£–∫–∞–∑–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            if critique_result.get("user_explanation", ""):
                explanation = critique_result["user_explanation"]
                if len(explanation) > 100:
                    score += 0.5
                if "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ" in explanation.lower() or "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫" in explanation.lower():
                    score += 0.5

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if critique_result.get("context_used"):
                score += 0.5

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ ECS: {e}")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É 1-5
        return max(1.0, min(5.0, score))

    def _generate_recommendations_with_metrics(self, analysis_result: Dict, query: str, context: Dict) -> Dict[
        str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
        logger.info("üí° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self._record_tool_invocation("RecommendationAgent")

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π RecommendationAgent
            result = self.recommendation_agent.process(
                analysis_results=analysis_result,
                user_context=context,
                query=query
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            if isinstance(result, dict):
                result.update({
                    "recommendation_timestamp": datetime.now().isoformat(),
                    "query": query
                })

            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return {
                "error": str(e),
                "fallback_recommendations": [
                    "–ò–∑—É—á–∏—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ",
                    "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"
                ]
            }

    def _recover_from_error(self, error: Exception, context: ExecutionContext) -> bool:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {error}")

        recovery_strategies = [
            self._simplify_query_strategy,
            self._fallback_search_strategy,
            self._cached_results_strategy
        ]

        for strategy in recovery_strategies:
            try:
                if strategy(context):
                    logger.info(f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ: {strategy.__name__}")
                    return True
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy.__name__} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                continue

        logger.error("‚ùå –í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∏—Å—å")
        return False

    def _simplify_query_strategy(self, context: ExecutionContext) -> bool:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–ø—Ä–æ—â–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        query = context.query
        if len(query.split()) > 3:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–ª–æ–≤–∞ –∫–∞–∫ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            simplified = " ".join(query.split()[:3])
            context.query = simplified
            context.context["simplified_query"] = simplified
            logger.info(f"üìù –£–ø—Ä–æ—â–µ–Ω –∑–∞–ø—Ä–æ—Å: '{simplified}'")
            return True
        return False

    def _fallback_search_strategy(self, context: ExecutionContext) -> bool:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–≥–∞–º"""
        context.context["search_method"] = "tags"
        context.context["force_tag_search"] = True
        logger.info("üè∑Ô∏è  –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º")
        return True

    def _cached_results_strategy(self, context: ExecutionContext) -> bool:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∏—Å—Ç–æ—Ä–∏–∏
        query_hash = hashlib.md5(context.query.lower().encode()).hexdigest()[:8]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        cache_file = self.output_dir / f"cache_{query_hash}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached = json.load(f)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                context.search_results = cached.get("results", [])
                context.context["cached_results"] = True
                logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
                return True
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")

        return False

    # ==================== –ú–ï–¢–†–ò–ö–ò –ò –û–¢–ß–ï–¢–ù–û–°–¢–¨ ====================

    def run_consistency_test(
            self,
            query: str,
            n_runs: int = 3,
            temperature: float = 0.5
    ) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (Self Consistency Score)

        Args:
            query: –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            n_runs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ (‚â• 3)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è LLM (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å > 0)

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞ —Å —Ä–∞—Å—á–µ—Ç–æ–º SCS
        """
        logger.info(f"üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: '{query}' (n={n_runs}, temp={temperature})")

        if n_runs < 3:
            n_runs = 3
            logger.warning(f"‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤ —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 3 (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏)")

        if temperature <= 0:
            temperature = 0.3
            logger.warning(f"‚ö†Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ 0.3 (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏)")

        all_results = []
        recommendations = []

        for run in range(n_runs):
            logger.info(f"üèÉ –ó–∞–ø—É—Å–∫ {run + 1}/{n_runs}")

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
            test_context = {
                "consistency_test": True,
                "test_run": run + 1,
                "temperature": temperature,
                "enable_critique": False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã —Ç–µ—Å—Ç–∞
            }

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            result = self.process_user_query(
                query=query,
                context=test_context,
                enable_critique=False,
                max_iterations=1
            )

            all_results.append(result)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            rec_text = self._extract_recommendations_text(result)
            recommendations.append(rec_text)

        # –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        consistency_score = self._calculate_consistency_score(recommendations)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∞
        test_result = {
            "query": query,
            "n_runs": n_runs,
            "temperature": temperature,
            "consistency_score": consistency_score,
            "recommendations_samples": recommendations[:2],
            "test_timestamp": datetime.now().isoformat(),
            "all_scores": [self._calculate_pairwise_similarity(r1, r2)
                           for r1, r2 in zip(recommendations[:-1], recommendations[1:])]
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –º–µ—Ç—Ä–∏–∫–∏
        self.metrics.consistency_tests.append(test_result)
        self._save_metrics()

        logger.info(f"‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω. SCS: {consistency_score:.3f}")

        return {
            "test_summary": test_result,
            "detailed_results": all_results
        }

    def _calculate_consistency_score(self, recommendations: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç Self Consistency Score –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        if len(recommendations) < 2:
            return 1.0

        similarities = []

        for i in range(len(recommendations)):
            for j in range(i + 1, len(recommendations)):
                sim = self._calculate_pairwise_similarity(recommendations[i], recommendations[j])
                similarities.append(sim)

        if not similarities:
            return 0.0

        return round(sum(similarities) / len(similarities), 3)

    def _calculate_pairwise_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ–ø–∞—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        if not text1 or not text2:
            return 0.0

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        words1 = set(re.findall(r'\b\w{3,}\b', text1.lower()))
        words2 = set(re.findall(r'\b\w{3,}\b', text2.lower()))

        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"—ç—Ç–æ", "—á—Ç–æ", "–∫–∞–∫", "–¥–ª—è", "–Ω–∞", "–ø–æ", "—Å", "–∏", "–≤", "–Ω–µ"}
        words1 = words1 - stop_words
        words2 = words2 - stop_words

        if not words1 or not words2:
            return 0.0

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return intersection / union if union > 0 else 0.0

    def _extract_recommendations_text(self, result: Dict) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            recs = result.get("final_recommendations", {})

            if isinstance(recs, dict):
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏
                for key in ["executive_summary", "summary", "recommendations", "answer"]:
                    if key in recs:
                        text = recs[key]
                        if isinstance(text, list):
                            return " ".join(str(item) for item in text)
                        return str(text)

            return str(recs)[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return ""

    def _record_tool_invocation(self, tool_name: str):
        """–ó–∞–ø–∏—Å—å –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ TID"""
        self.metrics.tool_invocations[tool_name] = self.metrics.tool_invocations.get(tool_name, 0) + 1

    def _update_metrics(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫"""
        # –ú–µ—Ç—Ä–∏–∫–∏ —É–∂–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        pass

    # ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ====================

    def _enrich_book_info(self, book: Dict) -> Dict:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        book_id = book.get('book_id', '')

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫—ç—à–∞
        enriched = book.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        summary = self._get_cached_summary(book_id)
        if summary:
            enriched['summary_preview'] = summary[:200] + "..." if len(summary) > 200 else summary

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'all_categories' not in enriched:
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö
            book_data = self.data_manager.get_book_by_id(book_id)
            if book_data:
                enriched['all_categories'] = {
                    'academic_subjects': book_data.get('academic_subjects', ''),
                    'genres': book_data.get('genres', ''),
                    'audience': book_data.get('audience', ''),
                    'complexity_level': book_data.get('complexity_level', '')
                }

        return enriched

    def _get_cached_book_text(self, book_id: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –±–∞–∑—ã"""
        if book_id in self._book_cache:
            return self._book_cache[book_id]

        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å
        book_info = self.data_manager.get_book_by_id(book_id)
        if book_info and 'file_path' in book_info:
            file_path = book_info['file_path']
            if os.path.exists(file_path):
                try:
                    result = self.pdf_processor.extract_text(file_path, max_pages=50)
                    if result.get('success'):
                        text = result.get('text', '')
                        self._book_cache[book_id] = text
                        return text
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")

        return None

    def _get_cached_summary(self, book_id: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ –∫–Ω–∏–≥–∏ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –±–∞–∑—ã"""
        if book_id in self._summary_cache:
            return self._summary_cache[book_id]

        summary = self.data_manager.get_summary(book_id)
        if summary:
            self._summary_cache[book_id] = summary
            return summary

        return None

    def _create_final_result(
            self,
            execution_context: ExecutionContext,
            final_recommendations: Optional[Dict],
            iteration_logs: List[Dict]
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""

        return {
            "execution_id": execution_context.execution_id,
            "query": execution_context.query,
            "timestamp": datetime.now().isoformat(),
            "processing_time": execution_context.processing_time,
            "final_decision": execution_context.final_decision.value if execution_context.final_decision else None,
            "iterations_count": len(iteration_logs),
            "books_found": len(execution_context.search_results) if execution_context.search_results else 0,
            "final_recommendations": final_recommendations,
            "metrics_snapshot": {
                "CER": round(self.metrics.critic_metrics.cer, 3),
                "TID": self.metrics.tid,
                "AAR": round(self.metrics.aar, 3),
                "SCS": round(self.metrics.scs, 3),
                "ECS": round(self.metrics.critic_metrics.average_ecs, 3),
                "FRR": round(self.metrics.frr, 3)
            },
            "iteration_summary": iteration_logs
        }

    def _save_execution_log(self, context: ExecutionContext, iteration_logs: List[Dict], result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        log_file = self.execution_logs_dir / f"{context.execution_id}.json"

        log_data = {
            "execution_id": context.execution_id,
            "query": context.query,
            "start_time": context.start_time.isoformat(),
            "end_time": datetime.now().isoformat(),
            "processing_time": context.processing_time,
            "final_decision": context.final_decision.value if context.final_decision else None,
            "iterations": iteration_logs,
            "result_summary": {
                "books_found": result.get("books_found", 0),
                "recommendations_generated": bool(result.get("final_recommendations"))
            },
            "context_keys": list(context.context.keys()) if context.context else []
        }

        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∞: {e}")

    def _load_metrics(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.metrics_file.exists():
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                # (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
                self.metrics.total_queries = data.get('total_queries', 0)
                self.metrics.accepted_answers = data.get('accepted_answers', 0)
                self.metrics.errors_detected = data.get('errors_detected', 0)
                self.metrics.errors_recovered = data.get('errors_recovered', 0)
                self.metrics.tool_invocations = defaultdict(int, data.get('tool_invocations', {}))

                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏: {self.metrics.total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏: {e}")

    def _save_metrics(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ —Ñ–∞–π–ª"""
        try:
            metrics_data = {
                "total_queries": self.metrics.total_queries,
                "accepted_answers": self.metrics.accepted_answers,
                "errors_detected": self.metrics.errors_detected,
                "errors_recovered": self.metrics.errors_recovered,
                "tool_invocations": dict(self.metrics.tool_invocations),
                "critic_metrics": {
                    "total_calls": self.metrics.critic_metrics.total_calls,
                    "effective_changes": self.metrics.critic_metrics.effective_changes,
                    "acceptance_count": self.metrics.critic_metrics.acceptance_count,
                    "average_ecs": self.metrics.critic_metrics.average_ecs
                },
                "consistency_tests": self.metrics.consistency_tests[-5:] if self.metrics.consistency_tests else [],
                "last_updated": datetime.now().isoformat()
            }

            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")

    # ==================== –ò–ù–¢–ï–†–§–ï–ô–° –î–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ====================

    def get_metrics_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫"""

        # –¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        current_metrics = {
            "CER": round(self.metrics.critic_metrics.cer, 3),
            "TID": self.metrics.tid,
            "AAR": round(self.metrics.aar, 3),
            "SCS": round(self.metrics.scs, 3),
            "ECS": round(self.metrics.critic_metrics.average_ecs, 3),
            "FRR": round(self.metrics.frr, 3)
        }

        # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
        requirements = {
            "CER": 0.2,
            "TID": 2,
            "AAR": 0.7,
            "SCS": 0.6,
            "ECS": 4.0,
            "FRR": 0.5
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        passed = {}
        for metric, value in current_metrics.items():
            passed[metric] = value >= requirements[metric]

        overall_passed = all(passed.values())

        return {
            "timestamp": datetime.now().isoformat(),
            "metrics": current_metrics,
            "requirements": requirements,
            "passed": passed,
            "overall_status": "‚úÖ –í–°–ï –ú–ï–¢–†–ò–ö–ò –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢" if overall_passed else "‚ö†Ô∏è –ù–ï–ö–û–¢–û–†–´–ï –ú–ï–¢–†–ò–ö–ò –ù–ò–ñ–ï –ü–û–†–û–ì–ê",
            "statistics": {
                "total_queries": self.metrics.total_queries,
                "average_processing_time": round(sum(self.metrics.execution_times) / len(self.metrics.execution_times),
                                                 2)
                if self.metrics.execution_times else 0.0,
                "total_books_in_db": len(self.data_manager.get_all_books()),
                "tools_used": list(self.metrics.tool_invocations.keys())
            }
        }

    def export_metrics_report(self, output_path: Optional[str] = None) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"""

        if output_path is None:
            output_path = self.output_dir / "metrics_report.md"
        else:
            output_path = Path(output_path)

        summary = self.get_metrics_summary()

        # –§–æ—Ä–º–∏—Ä—É–µ–º Markdown –æ—Ç—á–µ—Ç
        report = f"""# üìä –û–¢–ß–ï–¢ –ü–û –ú–ï–¢–†–ò–ö–ê–ú –°–ò–°–¢–ï–ú–´ –ê–ù–ê–õ–ò–ó–ê –ö–ù–ò–ì

## üìã –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- **–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤:** {summary['statistics']['total_queries']}
- **–ö–Ω–∏–≥ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:** {summary['statistics']['total_books_in_db']}
- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {summary['statistics']['average_processing_time']} —Å–µ–∫
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:** {', '.join(summary['statistics']['tools_used'])}

## üéØ –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã: **{summary['overall_status']}**

## üìà –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫

### A. –ê–ì–ï–ù–¢–ù–û–°–¢–¨

#### 1. Critic Effectiveness Rate (CER)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['CER']} | ‚â• {summary['requirements']['CER']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['CER'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –î–æ–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ CriticAgent —Ä–µ–∞–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–∏–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç) |

#### 2. Tool Invocation Diversity (TID)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['TID']} | ‚â• {summary['requirements']['TID']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['TID'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (–∞–≥–µ–Ω—Ç–æ–≤) –ø–æ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–µ LLM |

### B. –ö–ê–ß–ï–°–¢–í–û

#### 3. Answer Acceptance Rate (AAR)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['AAR']} | ‚â• {summary['requirements']['AAR']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['AAR'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –î–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–Ω—è—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–Ω–∞–ª–∏–∑–∞ |

#### 4. Self Consistency Score (SCS)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['SCS']} | ‚â• {summary['requirements']['SCS']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['SCS'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –°—Ç–µ–ø–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—É—Å–∫–∞—Ö —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º |
| **–ú–µ—Ç–æ–¥–∏–∫–∞:** 3+ –∑–∞–ø—É—Å–∫–∞ —Å temperature LLM > 0 |

#### 5. Explanation Completeness Score (ECS)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['ECS']} | ‚â• {summary['requirements']['ECS']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['ECS'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –ü–æ–ª–Ω–æ—Ç–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —Ä–µ—à–µ–Ω–∏–π (1-5 –±–∞–ª–ª–æ–≤) |
| **–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:** —É–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏—á–∏–Ω –≤—ã–±–æ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–∫–∞–∑–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π |

### C. –ù–ê–î–ï–ñ–ù–û–°–¢–¨

#### 6. Failure Recovery Rate (FRR)
| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—Ä–æ–≥ | –°—Ç–∞—Ç—É—Å |
|------------|----------|-------|--------|
| –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | {summary['metrics']['FRR']} | ‚â• {summary['requirements']['FRR']} | {'‚úÖ –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢' if summary['passed']['FRR'] else '‚ùå –ù–ò–ñ–ï –ü–û–†–û–ì–ê'} |
| **–û–ø–∏—Å–∞–Ω–∏–µ:** –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –∫ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ |

## üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤:
{self._format_agents_usage()}

### –ò—Å—Ç–æ—Ä–∏—è —Ç–µ—Å—Ç–æ–≤ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏:
{self._format_scs_history()}

## üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

{self._generate_improvement_recommendations(summary)}

---

*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏—Å—Ç–µ–º–æ–π PerfectBookAnalysisPipeline v1.0*
"""

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)

            logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            return ""

    def _format_agents_usage(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤"""
        if not self.metrics.tool_invocations:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤"

        lines = []
        for agent, count in sorted(self.metrics.tool_invocations.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"- **{agent}**: {count} –≤—ã–∑–æ–≤–æ–≤")

        return "\n".join(lines)

    def _format_scs_history(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ç–µ—Å—Ç–æ–≤ SCS"""
        if not self.metrics.consistency_tests:
            return "–¢–µ—Å—Ç—ã —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å"

        lines = []
        for i, test in enumerate(self.metrics.consistency_tests[-3:]):  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Ç–µ—Å—Ç–∞
            lines.append(f"{i + 1}. **{test.get('query', 'N/A')}**: SCS={test.get('consistency_score', 0):.3f} "
                         f"(n={test.get('n_runs', 0)}, temp={test.get('temperature', 0)})")

        return "\n".join(lines)

    def _generate_improvement_recommendations(self, summary: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫"""
        recommendations = []

        if not summary['passed']['CER']:
            recommendations.append("1. **–£–≤–µ–ª–∏—á–∏—Ç—å CER**: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å CriticAgent –Ω–∞ –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É, "
                                   "–¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")

        if not summary['passed']['TID']:
            recommendations.append("2. **–£–≤–µ–ª–∏—á–∏—Ç—å TID**: –ó–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã, "
                                   "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å CoordinatorAgent –Ω–∞ –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤.")

        if not summary['passed']['AAR']:
            recommendations.append("3. **–£–≤–µ–ª–∏—á–∏—Ç—å AAR**: –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, "
                                   "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π CriticAgent.")

        if not summary['passed']['SCS']:
            recommendations.append("4. **–£–≤–µ–ª–∏—á–∏—Ç—å SCS**: –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–ø—Ä–æ—Å—ã, "
                                   "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤.")

        if not summary['passed']['ECS']:
            recommendations.append("5. **–£–≤–µ–ª–∏—á–∏—Ç—å ECS**: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å CriticAgent –Ω–∞ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è, "
                                   "–¥–æ–±–∞–≤–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –æ—Ç–≤–µ—Ç—ã (–ø—Ä–∏—á–∏–Ω—ã –≤—ã–±–æ—Ä–∞, –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è).")

        if not summary['passed']['FRR']:
            recommendations.append("6. **–£–≤–µ–ª–∏—á–∏—Ç—å FRR**: –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è, "
                                   "—É–ª—É—á—à–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∏—Å–∫–ª—é—á–µ–Ω–∏–π, —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–æ–∏—Å–∫–∞.")

        if not recommendations:
            recommendations.append("‚úÖ –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å "
                                   "—Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã.")

        return "\n\n".join(recommendations)

    def clear_metrics(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
        self.metrics = PipelineMetrics()
        self._book_cache.clear()
        self._summary_cache.clear()

        # –û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–µ—Ç—Ä–∏–∫
        if self.metrics_file.exists():
            self.metrics_file.unlink()

        logger.info("üßπ –ú–µ—Ç—Ä–∏–∫–∏ –æ—á–∏—â–µ–Ω—ã")

    # ==================== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° –°–£–©–ï–°–¢–í–£–Æ–©–ï–ô –°–ò–°–¢–ï–ú–û–ô ====================

    def process_book_upload(
            self,
            pdf_path: str,
            title: Optional[str] = None,
            generate_summary: bool = True,
            generate_tags: bool = True
    ) -> Dict[str, Any]:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–Ω–∏–≥.

        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
            generate_summary: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ —Ä–µ–∑—é–º–µ
            generate_tags: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ —Ç–µ–≥–∏

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–Ω–∏–≥–∏
        """
        logger.info(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–Ω–∏–≥–∏: {pdf_path}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –∏–∑ main.py
        from main import SmartLibrarySystem

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        temp_system = SmartLibrarySystem(verify_ssl=False)

        # –í—ã–∑—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏
        result = temp_system.upload_book(pdf_path, title)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
        if result.get("success") and "book_id" in result:
            book_id = result["book_id"]

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
            if generate_tags:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø–æ–∏—Å–∫
                book_data = self.data_manager.get_book_by_id(book_id)
                if book_data:
                    self.tag_search.add_book_tags(book_data)

            # –û—á–∏—â–∞–µ–º –∫—ç—à–∏
            self._book_cache.pop(book_id, None)
            self._summary_cache.pop(book_id, None)

        return result


# ==================== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ô –ë–õ–û–ö ====================

def demo_pipeline():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""

    print("=" * 80)
    print("üöÄ –ò–î–ï–ê–õ–¨–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ê–ù–ê–õ–ò–ó–ê –ö–ù–ò–ì - –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø")
    print("=" * 80)

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("\nüîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        pipeline = PerfectBookAnalysisPipeline(
            base_data_dir="uploads",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            output_dir="perfect_pipeline_output",
            verify_ssl=False
        )

        print("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        # –û—Å–Ω–æ–≤–Ω–æ–µ –º–µ–Ω—é
        while True:
            print("\n" + "=" * 80)
            print("üìö –ì–õ–ê–í–ù–û–ï –ú–ï–ù–Æ:")
            print("1. üîç –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å")
            print("2. üß™ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (SCS)")
            print("3. üì§ –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –∫–Ω–∏–≥—É (PDF)")
            print("4. üìä –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏")
            print("5. üìÑ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º")
            print("6. üßπ –û—á–∏—Å—Ç–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)")
            print("7. üö™ –í—ã–π—Ç–∏")

            choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-7): ").strip()

            if choice == "1":
                query = input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä: '—É—á–µ–±–Ω–∏–∫ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é'): ").strip()
                if query:
                    print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
                    result = pipeline.process_user_query(query, enable_critique=True)

                    print(f"\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢:")
                    print(f"ID –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.get('execution_id')}")
                    print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.get('processing_time', 0):.2f} —Å–µ–∫")
                    print(f"–ò—Ç–µ—Ä–∞—Ü–∏–π: {result.get('iterations_count', 0)}")
                    print(f"–ö–Ω–∏–≥ –Ω–∞–π–¥–µ–Ω–æ: {result.get('books_found', 0)}")

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                    recs = result.get('final_recommendations', {})
                    if recs and isinstance(recs, dict):
                        if 'executive_summary' in recs:
                            print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                            print(recs['executive_summary'][:300] + "...")

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    metrics = result.get('metrics_snapshot', {})
                    if metrics:
                        print(f"\nüìà –ú–ï–¢–†–ò–ö–ò –≠–¢–û–ì–û –ó–ê–ü–†–û–°–ê:")
                        for metric, value in metrics.items():
                            print(f"  {metric}: {value}")

            elif choice == "2":
                test_query = "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö"
                print(f"\nüß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: '{test_query}'")
                print("(–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è 3 –∑–∞–ø—É—Å–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏)")

                result = pipeline.run_consistency_test(
                    query=test_query,
                    n_runs=3,
                    temperature=0.5
                )

                test_summary = result['test_summary']
                print(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω:")
                print(f"SCS: {test_summary['consistency_score']:.3f}")
                print(f"–ó–∞–ø—É—Å–∫–æ–≤: {test_summary['n_runs']}")
                print(f"Temperature LLM: {test_summary['temperature']}")

            elif choice == "3":
                file_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É: ").strip()
                if os.path.exists(file_path):
                    result = pipeline.process_book_upload(file_path)

                    if result.get("success"):
                        print(f"\n‚úÖ –ö–Ω–∏–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞:")
                        print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('title')}")
                        print(f"ID: {result.get('book_id')}")
                        print(
                            f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.get('processing_summary', {}).get('text_extracted', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
                    else:
                        print(f"\n‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                else:
                    print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")

            elif choice == "4":
                metrics = pipeline.get_metrics_summary()

                print("\nüìä –¢–ï–ö–£–©–ò–ï –ú–ï–¢–†–ò–ö–ò –ü–ê–ô–ü–õ–ê–ô–ù–ê:")
                print(f"–°—Ç–∞—Ç—É—Å: {metrics['overall_status']}")
                print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['statistics']['total_queries']}")

                print("\n–ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫:")
                for metric, value in metrics['metrics'].items():
                    status = "‚úÖ" if metrics['passed'][metric] else "‚ùå"
                    print(f"{status} {metric}: {value} (–ø–æ—Ä–æ–≥: {metrics['requirements'][metric]})")

            elif choice == "5":
                report_path = pipeline.export_metrics_report()
                if report_path:
                    print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
                    print("–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª –≤ Markdown-—Ä–µ–¥–∞–∫—Ç–æ—Ä–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞")
                else:
                    print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞")

            elif choice == "6":
                confirm = input("‚ö†Ô∏è  –í—ã —É–≤–µ—Ä–µ–Ω—ã? –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã (y/N): ").strip().lower()
                if confirm == 'y':
                    pipeline.clear_metrics()
                    print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –æ—á–∏—â–µ–Ω—ã")

            elif choice == "7":
                print("\nüëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
                break

            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

    except KeyboardInterrupt:
        print("\n\nüëã –í—ã—Ö–æ–¥ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    demo_pipeline()
